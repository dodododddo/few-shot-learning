# few-shot-learning
## 一、总览

少样本学习研究的是如何利用极有限的标注样本解决目标任务。这里的 “少” 指的是和任务直接相关的样本少，同样需要大量的、与任务不直接相关的数据来为模型提供通用的、可泛化的知识来完成任务。

以少样本图像分类为例，其常见的N-way-K-shot的任务设置基于这样一种假设：人们希望将一些不常见的图片分为N个类，但只有极少量的图片作为示例（如每个类只有K张示例图）。如果直接用这N * K张图片训练模型，由于数据量过少，很难完成任务。一个自然的想法是，尽管没有更多的关于这些类别图片的示意图，却有很多其他的标注图片可以利用。这些图片中蕴含着一些通用的、有利于提取图像特征的知识，这也同样有利于原分类任务的完成。因此，从抽象的意义上讲，少样本图像分类真正关心的问题有二，一是寻找更具泛化性的图像表示，二是更好利用训练好的模型中关于图像的知识。

少样本学习的提出时间较早，已不是一个新鲜的领域。随着预训练范式的兴起，少样本这一概念事实上已变得模糊。预训练大模型追求的是用超大量数据训练一个通用模型，通过提示或少量微调便能很好完成具体任务，而不必为每个任务单独寻找数据、训练模型。从使用的角度看，GPT天然就是Few-Shot甚至Zero-Shot的，只需要极少量的提示就可以让GPT学会按固定格式答题，还支持各种各样、不同形式的问题。如果算上GPT-V，可以认为目前所有的少样本任务都可以被预训练模型很好地解决；从训练的角度看，各种大模型使用的数据是如此庞大，区分数据是否在训练数据中、与训练数据的分布是否一致已经几乎是无意义的行为了。从这一意义上讲，目前少样本学习的任务设置实际上是一个solved problem，在少样本数据集上的各种指标的意义上是极为有限的。

但从另一个角度讲，少样本学习事实上是以一种极端的设置研究如何获得更具泛化性的知识表示，如何更好地引导、利用、迁移模型中的知识，它的任务本身没有太大的意义，其作用在于让你无须也无法考虑其他内容，直接面对这两个永不过时的主题。从这个意义上讲，Few-Shot是一个仍有意思的虚构场景。

综合以上几点，本人对少样本图像分类相关研究的态度是：主题是有趣的，但具体任务设置本身还有很多可以探索的地方。

传统的在MiniImageNet等数据集上划分新类基类的任务设置确实没有太大意义了。近年来在任务设置上有一些创新，如跨域、无监督预训练、直接统一使用CLIP作为与基础encoder。

从一个很诚实的视角出发：少样本图像分类研究中的大多数方法事实上是很难scale-up的，这并不是少样本学习领域特有的问题，如先前语言模型处理长上下文的研究，在模型scale up后仍然起效的并不多，LLAMA也没有采用专门处理长上下文的特化结构。

因此，跨域这一设置尽管提高了理论意义，但如果提出的方法本身并无扎实的理论支撑或实验来证明其Scale-up能力，那仍没有根本上改变传统任务设置的问题。而使用CLIP作为统一基础encoder这样的设置，将少样本学习研究的主题彻底转向了如何利用知识，不管从研究成本（研究表示可能需要微调）还是从实用性来考虑，都是一个更有意义的任务设置。这方面的工作有CLIP Adapter, TIP Adapter, APE等。

当然，“更实用”并非是唯一的出路。另一条路的代表工作是《Universal few-Shot learner for dense prediction tasks》，它获得了ICLR2023的杰出论文。如果我们坚持认为少样本学习的价值依然在于探究泛化这一核心价值，那么一切过分关注具体任务、依赖具体任务先验知识的方法都在一定程度上有悖于这一初衷。这篇论文提出了一种通用视觉任务的少样本学习方法，最大程度减少了少样本方法与特定任务先验知识的耦合，进而能探究少样本学习的原本关注的核心内容。

当然了，我进入这个领域仅仅4个月左右，坐而论道、大谈研究品味绝不是我该做与能做的。以上仅仅是个人的一些随想，本文的重心仍在于梳理少样本图像分类领域的个人关注到的不同切入角度和一些代表性的工作。





#### 按模态分
* Vision
* Language
* Multimodal

#### 按任务分
* Classfication
* Segmentation
* Detection
* Generation
* Extraction
* ...

###具体到少样本图像分类任务：
#### 按可利用的数据类型分
* 大量非目标带标签样本 + 少量目标带标签样本
* 海量无标签样本 + 少量目标带标签样本
* CLIP + 少量目标带标签样本
* ...

#### 按预训练数据与目标任务的跨度分
* 域内少样本学习
* 跨域少样本学习

## 二、少样本图像分类的基本思路
#### 基本框架: encoder + classfier-head
#### 常见思路与代表工作：(TODO)









